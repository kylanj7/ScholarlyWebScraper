{
  "query": "deep neural networks",
  "timestamp": "2025-12-29T09:23:21.436547",
  "total_found": 9,
  "downloaded": 5,
  "papers": [
    {
      "title": "The Deep Arbitrary Polynomial Chaos Neural Network or how Deep Artificial Neural Networks could benefit from Data-Driven Homogeneous Chaos Theory",
      "authors": "Sergey Oladyshkin, Timothy Praditia, Ilja Kröker, Farid Mohammadi, Wolfgang Nowak, Sebastian Otte",
      "abstract": "Artificial Intelligence and Machine learning have been widely used in various fields of mathematical computing, physical modeling, computational science, communication science, and stochastic analysis. Approaches based on Deep Artificial Neural Networks (DANN) are very popular in our days. Depending on the learning task, the exact form of DANNs is determined via their multi-layer architecture, activation functions and the so-called loss function. However, for a majority of deep learning approaches based on DANNs, the kernel structure of neural signal processing remains the same, where the node response is encoded as a linear superposition of neural activity, while the non-linearity is triggered by the activation functions. In the current paper, we suggest to analyze the neural signal processing in DANNs from the point of view of homogeneous chaos theory as known from polynomial chaos expansion (PCE). From the PCE perspective, the (linear) response on each node of a DANN could be seen as a $1^{st}$ degree multi-variate polynomial of single neurons from the previous layer, i.e. linear weighted sum of monomials. From this point of view, the conventional DANN structure relies implicitly (but erroneously) on a Gaussian distribution of neural signals. Additionally, this view revels that by design DANNs do not necessarily fulfill any orthogonality or orthonormality condition for a majority of data-driven applications. Therefore, the prevailing handling of neural signals in DANNs could lead to redundant representation as any neural signal could contain some partial information from other neural signals. To tackle that challenge, we suggest to employ the data-driven generalization of PCE theory known as arbitrary polynomial chaos (aPC) to construct a corresponding multi-variate orthonormal representations on each node of a DANN to obtain Deep arbitrary polynomial chaos neural networks.",
      "pdf_url": "https://arxiv.org/pdf/2306.14753v1",
      "source": "ArXiv",
      "query": "deep neural networks"
    },
    {
      "title": "On the approximation of rough functions with deep neural networks",
      "authors": "Tim De Ryck, Siddhartha Mishra, Deep Ray",
      "abstract": "Deep neural networks and the ENO procedure are both efficient frameworks for approximating rough functions. We prove that at any order, the ENO interpolation procedure can be cast as a deep ReLU neural network. This surprising fact enables the transfer of several desirable properties of the ENO procedure to deep neural networks, including its high-order accuracy at approximating Lipschitz functions. Numerical tests for the resulting neural networks show excellent performance for approximating solutions of nonlinear conservation laws and at data compression.",
      "pdf_url": "https://arxiv.org/pdf/1912.06732v2",
      "source": "ArXiv",
      "query": "deep neural networks"
    },
    {
      "title": "Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks",
      "authors": "Danny D'Agostino, Ilija Ilievski, Christine Annette Shoemaker",
      "abstract": "Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the prediction task enhancing the model interpretability. We conducted numerical experiments for regression, classification, and feature selection tasks, comparing our model against popular machine learning models, the state-of-the-art deep learning-based embedding feature selection techniques, and a transformer model for tabular data. Our results demonstrate that the proposed model does not only yield an attractive prediction performance compared to the competitors but also provides meaningful and interpretable results that potentially could assist the decision-making process in real-world applications. A PyTorch implementation of the model is available on GitHub at the following link. https://github.com/dannyzx/Gaussian-RBFNN",
      "pdf_url": "https://arxiv.org/pdf/2307.05639v2",
      "source": "ArXiv",
      "query": "deep neural networks"
    },
    {
      "title": "Improving Generalization of Deep Neural Networks by Leveraging Margin Distribution",
      "authors": "Shen-Huan Lyu, Lu Wang, Zhi-Hua Zhou",
      "abstract": "Recent research has used margin theory to analyze the generalization performance for deep neural networks (DNNs). The existed results are almost based on the spectrally-normalized minimum margin. However, optimizing the minimum margin ignores a mass of information about the entire margin distribution, which is crucial to generalization performance. In this paper, we prove a generalization upper bound dominated by the statistics of the entire margin distribution. Compared with the minimum margin bounds, our bound highlights an important measure for controlling the complexity, which is the ratio of the margin standard deviation to the expected margin. We utilize a convex margin distribution loss function on the deep neural networks to validate our theoretical results by optimizing the margin ratio. Experiments and visualizations confirm the effectiveness of our approach and the correlation between generalization gap and margin ratio.",
      "pdf_url": "https://arxiv.org/pdf/1812.10761v3",
      "source": "ArXiv",
      "query": "deep neural networks"
    },
    {
      "title": "Hierarchical Attentional Hybrid Neural Networks for Document Classification",
      "authors": "Jader Abreu, Luis Fred, David Macêdo, Cleber Zanchettin",
      "abstract": "Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classification tasks. The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical representation. The proposed method in this paper improves the results of the current attention-based approaches for document classification.",
      "pdf_url": "https://arxiv.org/pdf/1901.06610v2",
      "source": "ArXiv",
      "query": "deep neural networks"
    },
    {
      "title": "Deep-gKnock: Nonlinear group-feature selection with deep neural networks",
      "authors": "Guangyu Zhu, Tingting Zhao",
      "abstract": "",
      "doi": "10.1016/j.neunet.2020.12.004",
      "publisher": "Elsevier BV",
      "year": 2021,
      "url": "https://doi.org/10.1016/j.neunet.2020.12.004",
      "source": "CrossRef",
      "query": "deep neural networks"
    },
    {
      "title": "Training Deep Neural Networks",
      "authors": "Charu C. Aggarwal",
      "abstract": "",
      "doi": "10.1007/978-3-319-94463-0_3",
      "publisher": "Springer International Publishing",
      "year": 2018,
      "url": "https://doi.org/10.1007/978-3-319-94463-0_3",
      "source": "CrossRef",
      "query": "deep neural networks"
    },
    {
      "title": "Stability of Deep Neural Networks via Discrete Rough Paths",
      "authors": "",
      "abstract": "",
      "doi": "10.37473/fic/10.1137/22m1472358",
      "publisher": "Rescognito, Inc.",
      "year": null,
      "url": "https://doi.org/10.37473/fic/10.1137/22m1472358",
      "source": "CrossRef",
      "query": "deep neural networks"
    },
    {
      "title": "Correction to: Neural Networks and Deep Learning",
      "authors": "Charu Aggarwal",
      "abstract": "",
      "doi": "10.1007/978-3-031-29642-0_13",
      "publisher": "Springer International Publishing",
      "year": 2023,
      "url": "https://doi.org/10.1007/978-3-031-29642-0_13",
      "source": "CrossRef",
      "query": "deep neural networks"
    }
  ]
}