{
  "query": "transformer models attention mechanism",
  "timestamp": "2025-12-29T09:24:09.519735",
  "total_found": 10,
  "downloaded": 5,
  "papers": [
    {
      "title": "Transformer-based Personalized Attention Mechanism for Medical Images with Clinical Records",
      "authors": "Yusuke Takagi, Noriaki Hashimoto, Hiroki Masuda, Hiroaki Miyoshi, Koichi Ohshima, Hidekata Hontani, Ichiro Takeuchi",
      "abstract": "In medical image diagnosis, identifying the attention region, i.e., the region of interest for which the diagnosis is made, is an important task. Various methods have been developed to automatically identify target regions from given medical images. However, in actual medical practice, the diagnosis is made based not only on the images but also on a variety of clinical records. This means that pathologists examine medical images with some prior knowledge of the patients and that the attention regions may change depending on the clinical records. In this study, we propose a method called the Personalized Attention Mechanism (PersAM), by which the attention regions in medical images are adaptively changed according to the clinical records. The primary idea of the PersAM method is to encode the relationships between the medical images and clinical records using a variant of Transformer architecture. To demonstrate the effectiveness of the PersAM method, we applied it to a large-scale digital pathology problem of identifying the subtypes of 842 malignant lymphoma patients based on their gigapixel whole slide images and clinical records.",
      "pdf_url": "https://arxiv.org/pdf/2206.03003v2",
      "source": "ArXiv",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Dilated Neighborhood Attention Transformer",
      "authors": "Ali Hassani, Humphrey Shi",
      "abstract": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
      "pdf_url": "https://arxiv.org/pdf/2209.15001v3",
      "source": "ArXiv",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
      "authors": "Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, Jiaya Jia",
      "abstract": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
      "pdf_url": "https://arxiv.org/pdf/2309.01692v1",
      "source": "ArXiv",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Music Transformer",
      "authors": "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, Douglas Eck",
      "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
      "pdf_url": "https://arxiv.org/pdf/1809.04281v3",
      "source": "ArXiv",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Déjà vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation",
      "authors": "Jibang Wu, Renqin Cai, Hongning Wang",
      "abstract": "Predicting users' preferences based on their sequential behaviors in history is challenging and crucial for modern recommender systems. Most existing sequential recommendation algorithms focus on transitional structure among the sequential actions, but largely ignore the temporal and context information, when modeling the influence of a historical event to current prediction.\n  In this paper, we argue that the influence from the past events on a user's current action should vary over the course of time and under different context. Thus, we propose a Contextualized Temporal Attention Mechanism that learns to weigh historical actions' influence on not only what action it is, but also when and how the action took place. More specifically, to dynamically calibrate the relative input dependence from the self-attention mechanism, we deploy multiple parameterized kernel functions to learn various temporal dynamics, and then use the context information to determine which of these reweighing kernels to follow for each input. In empirical evaluations on two large public recommendation datasets, our model consistently outperformed an extensive set of state-of-the-art sequential recommendation methods.",
      "pdf_url": "https://arxiv.org/pdf/2002.00741v1",
      "source": "ArXiv",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "The Triple Attention Transformer: Advancing Contextual Coherence in Transformer Models",
      "authors": "Shadi Ghaith",
      "abstract": "<title>Abstract</title>\n        <p>This paper introduces the Triple Attention Transformer (TAT), a transformative approach in transformer models, tailored for enhancing long-term contextual coherence in dialogue systems. TAT innovates by representing dialogues as chunks of sequences, coupled with a triple attention mechanism. This novel architecture enables TAT to effectively manage extended sequences, addressing the coherence challenges inherent in traditional transformer models. Empirical evaluations using the Schema-Guided Dialogue Dataset from DSTC8 demonstrate TAT's enhanced performance, with significant improvements in Character Error Rate, Word Error Rate, and BLEU score. Importantly, TAT excels in generating coherent, extended dialogues, showcasing its advanced contextual comprehension. The integration of Conv1D networks, dual-level positional encoding, and decayed attention weighting are pivotal to TAT's robust context management. The paper also highlights the BERT variant of TAT, which leverages pre-trained language models to further enrich dialogue understanding and generation capabilities. Future developments include refining attention mechanisms, improving role distinction, and architectural optimizations. TAT's applicability extends to various complex NLP tasks, affirming its potential as a pioneering advancement in natural language processing.</p>",
      "doi": "10.21203/rs.3.rs-3916608/v1",
      "publisher": "Springer Science and Business Media LLC",
      "year": null,
      "url": "https://doi.org/10.21203/rs.3.rs-3916608/v1",
      "source": "CrossRef",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Generalized Attention Mechanism and Relative Position for Transformer",
      "authors": "Raja Vikram Pandya",
      "abstract": "",
      "doi": "10.31224/2476",
      "publisher": "Open Engineering Inc",
      "year": null,
      "url": "https://doi.org/10.31224/2476",
      "source": "CrossRef",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Combining Transformer and Reverse Attention Mechanism for Polyp Segmentation",
      "authors": "Jianzhuang Lin, Wenzhong Yang, Sixiang Tan",
      "abstract": "",
      "doi": "10.5220/0012014800003633",
      "publisher": "SCITEPRESS - Science and Technology Publications",
      "year": 2022,
      "url": "https://doi.org/10.5220/0012014800003633",
      "source": "CrossRef",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "Table 6: Automatic evaluation scores of GRU based Seq2Seq with attention and Transformer models.",
      "authors": "",
      "abstract": "",
      "doi": "10.7717/peerj-cs.3072/table-6",
      "publisher": "PeerJ",
      "year": null,
      "url": "https://doi.org/10.7717/peerj-cs.3072/table-6",
      "source": "CrossRef",
      "query": "transformer models attention mechanism"
    },
    {
      "title": "",
      "authors": "",
      "abstract": "",
      "doi": "10.1117/12.3047444.817754ca-1755-ef11-a9a2-00505691c5e1",
      "publisher": "SPIE-Intl Soc Optical Eng",
      "year": null,
      "url": "https://doi.org/10.1117/12.3047444.817754ca-1755-ef11-a9a2-00505691c5e1",
      "source": "CrossRef",
      "query": "transformer models attention mechanism"
    }
  ]
}